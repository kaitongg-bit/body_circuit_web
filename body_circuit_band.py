"""
èº«ä½“å›è·¯ä¹é˜Ÿ (Body Circuit Band)
ä½¿ç”¨å§¿æ€è¯†åˆ«å’Œäººä½“äº’åŠ¨æ§åˆ¶éŸ³ä¹æ’­æ”¾çš„äº’åŠ¨è£…ç½®
"""

import cv2
import mediapipe as mp
import numpy as np
import pygame
import math
from typing import List, Tuple, Optional
from dataclasses import dataclass


@dataclass
class Person:
    """äººç‰©å§¿æ€æ•°æ®"""
    x_center: float  # ç”¨äºæ’åº
    left_wrist: Tuple[float, float]
    right_wrist: Tuple[float, float]
    left_shoulder: Tuple[float, float]
    right_shoulder: Tuple[float, float]
    person_id: str  # 'A', 'B', 'C'

    def is_hands_raised(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸¾æ‰‹ï¼ˆä¸¤åªæ‰‹éƒ½é«˜äºè‚©è†€ï¼‰"""
        left_raised = self.left_wrist[1] < self.left_shoulder[1]
        right_raised = self.right_wrist[1] < self.right_shoulder[1]
        return left_raised and right_raised


class CircuitDetector:
    """ç”µè·¯é—­åˆæ£€æµ‹å™¨ï¼ˆå¸¦é˜²æŠ–ï¼‰"""

    def __init__(self, distance_threshold: float = 0.15, debounce_frames: int = 10):
        self.distance_threshold = distance_threshold
        self.debounce_frames = debounce_frames
        self.closed_count = 0
        self.open_count = 0
        self.current_state = False

    def calculate_distance(self, p1: Tuple[float, float], p2: Tuple[float, float]) -> float:
        """è®¡ç®—ä¸¤ç‚¹æ¬§å¼è·ç¦»"""
        return math.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)

    def check_circuit(self, person_a: Person, person_b: Person, person_c: Person) -> Tuple[bool, float, List[float]]:
        """
        æ£€æŸ¥ç”µè·¯æ˜¯å¦é—­åˆ
        è¿”å›: (æ˜¯å¦é—­åˆ, å¹³å‡è·ç¦», [d1, d2, d3])
        """
        # æ£€æŸ¥æ‰€æœ‰äººæ˜¯å¦ä¸¾æ‰‹
        if not (person_a.is_hands_raised() and person_b.is_hands_raised() and person_c.is_hands_raised()):
            return False, 1.0, [1.0, 1.0, 1.0]

        # è®¡ç®—ä¸‰å¯¹æ‰‹è…•è·ç¦»
        # A å³æ‰‹ â†” B å·¦æ‰‹
        d1 = self.calculate_distance(person_a.right_wrist, person_b.left_wrist)
        # B å³æ‰‹ â†” C å·¦æ‰‹
        d2 = self.calculate_distance(person_b.right_wrist, person_c.left_wrist)
        # C å³æ‰‹ â†” A å·¦æ‰‹
        d3 = self.calculate_distance(person_c.right_wrist, person_a.left_wrist)

        distances = [d1, d2, d3]
        d_avg = sum(distances) / 3

        # åˆ¤æ–­æ˜¯å¦æ‰€æœ‰è·ç¦»éƒ½å°äºé˜ˆå€¼
        all_close = all(d < self.distance_threshold for d in distances)

        return all_close, d_avg, distances

    def update(self, person_a: Person, person_b: Person, person_c: Person) -> Tuple[bool, float, List[float]]:
        """
        æ›´æ–°çŠ¶æ€ï¼ˆå¸¦é˜²æŠ–ï¼‰
        è¿”å›: (å½“å‰ç¨³å®šçŠ¶æ€, å¹³å‡è·ç¦», [d1, d2, d3])
        """
        instant_closed, d_avg, distances = self.check_circuit(person_a, person_b, person_c)

        if instant_closed:
            self.closed_count += 1
            self.open_count = 0
            if self.closed_count >= self.debounce_frames and not self.current_state:
                self.current_state = True
        else:
            self.open_count += 1
            self.closed_count = 0
            if self.open_count >= self.debounce_frames and self.current_state:
                self.current_state = False

        return self.current_state, d_avg, distances


class AudioController:
    """éŸ³é¢‘æ§åˆ¶å™¨"""

    def __init__(self, drum_path: str, bass_path: str, harmony_path: str):
        pygame.mixer.init(frequency=44100, size=-16, channels=2, buffer=512)

        # åŠ è½½éŸ³è½¨
        self.drum = pygame.mixer.Sound(drum_path)
        self.bass = pygame.mixer.Sound(bass_path)
        self.harmony = pygame.mixer.Sound(harmony_path)

        self.tracks = [self.drum, self.bass, self.harmony]
        self.is_playing = False
        self.channels = []

    def start_playback(self):
        """å¼€å§‹æ’­æ”¾æ‰€æœ‰éŸ³è½¨"""
        if not self.is_playing:
            self.channels = []
            for track in self.tracks:
                channel = track.play(loops=-1)  # å¾ªç¯æ’­æ”¾
                self.channels.append(channel)
            self.is_playing = True
            print("ğŸµ éŸ³ä¹å¼€å§‹æ’­æ”¾")

    def stop_playback(self):
        """åœæ­¢æ’­æ”¾æ‰€æœ‰éŸ³è½¨"""
        if self.is_playing:
            for track in self.tracks:
                track.stop()
            self.channels = []
            self.is_playing = False
            print("ğŸ”‡ éŸ³ä¹åœæ­¢")

    def set_volume(self, distance_avg: float, max_distance: float = 0.15):
        """æ ¹æ®å¹³å‡è·ç¦»æ§åˆ¶éŸ³é‡ï¼ˆè·ç¦»è¶Šè¿‘éŸ³é‡è¶Šå¤§ï¼‰"""
        if self.is_playing and self.channels:
            # è·ç¦»è¶Šå°ï¼ŒéŸ³é‡è¶Šå¤§
            volume = max(0.0, min(1.0, 1.0 - (distance_avg / max_distance)))
            for channel in self.channels:
                if channel:
                    channel.set_volume(volume)

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.stop_playback()
        pygame.mixer.quit()


class VisualFeedback:
    """è§†è§‰åé¦ˆç»˜åˆ¶"""

    @staticmethod
    def draw_person_landmarks(frame, person: Person, color: Tuple[int, int, int], frame_width: int, frame_height: int):
        """ç»˜åˆ¶äººç‰©å…³é”®ç‚¹"""
        # è½¬æ¢å½’ä¸€åŒ–åæ ‡åˆ°åƒç´ åæ ‡
        points = {
            'left_wrist': person.left_wrist,
            'right_wrist': person.right_wrist,
            'left_shoulder': person.left_shoulder,
            'right_shoulder': person.right_shoulder
        }

        for name, (x, y) in points.items():
            px, py = int(x * frame_width), int(y * frame_height)
            cv2.circle(frame, (px, py), 8, color, -1)

        # ç»˜åˆ¶äººç‰©æ ‡è¯†
        text_x = int(person.x_center * frame_width)
        cv2.putText(frame, person.person_id, (text_x, 50),
                   cv2.FONT_HERSHEY_SIMPLEX, 1.5, color, 3)

    @staticmethod
    def draw_connections(frame, person_a: Person, person_b: Person, person_c: Person,
                        distances: List[float], circuit_closed: bool,
                        frame_width: int, frame_height: int, threshold: float):
        """ç»˜åˆ¶è¿æ¥çº¿"""
        connections = [
            (person_a.right_wrist, person_b.left_wrist, distances[0]),
            (person_b.right_wrist, person_c.left_wrist, distances[1]),
            (person_c.right_wrist, person_a.left_wrist, distances[2])
        ]

        for (p1, p2, dist) in connections:
            x1, y1 = int(p1[0] * frame_width), int(p1[1] * frame_height)
            x2, y2 = int(p2[0] * frame_width), int(p2[1] * frame_height)

            # æ ¹æ®è·ç¦»å’Œç”µè·¯çŠ¶æ€é€‰æ‹©é¢œè‰²å’Œç²—ç»†
            if circuit_closed:
                color = (0, 255, 0)  # ç»¿è‰²
                thickness = 6
            elif dist < threshold:
                color = (0, 255, 255)  # é»„è‰²
                thickness = 4
            else:
                color = (0, 0, 255)  # çº¢è‰²
                thickness = 2

            cv2.line(frame, (x1, y1), (x2, y2), color, thickness)

            # æ˜¾ç¤ºè·ç¦»
            mid_x, mid_y = (x1 + x2) // 2, (y1 + y2) // 2
            cv2.putText(frame, f"{dist:.3f}", (mid_x, mid_y),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)


class BodyCircuitBand:
    """èº«ä½“å›è·¯ä¹é˜Ÿä¸»ç±»"""

    def __init__(self, drum_path: str, bass_path: str, harmony_path: str):
        # åˆå§‹åŒ– MediaPipe
        self.mp_pose = mp.solutions.pose
        self.pose = self.mp_pose.Pose(
            static_image_mode=False,
            model_complexity=1,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )

        # åˆå§‹åŒ–ç»„ä»¶
        self.circuit_detector = CircuitDetector(distance_threshold=0.15, debounce_frames=10)
        self.audio_controller = AudioController(drum_path, bass_path, harmony_path)
        self.visual = VisualFeedback()

        # çŠ¶æ€
        self.previous_circuit_state = False

    def extract_person_data(self, results, frame_width: int, frame_height: int) -> Optional[Person]:
        """ä» MediaPipe ç»“æœæå–äººç‰©æ•°æ®"""
        if not results.pose_landmarks:
            return None

        landmarks = results.pose_landmarks.landmark

        # è·å–å…³é”®ç‚¹
        left_wrist = (landmarks[self.mp_pose.PoseLandmark.LEFT_WRIST].x,
                     landmarks[self.mp_pose.PoseLandmark.LEFT_WRIST].y)
        right_wrist = (landmarks[self.mp_pose.PoseLandmark.RIGHT_WRIST].x,
                      landmarks[self.mp_pose.PoseLandmark.RIGHT_WRIST].y)
        left_shoulder = (landmarks[self.mp_pose.PoseLandmark.LEFT_SHOULDER].x,
                        landmarks[self.mp_pose.PoseLandmark.LEFT_SHOULDER].y)
        right_shoulder = (landmarks[self.mp_pose.PoseLandmark.RIGHT_SHOULDER].x,
                         landmarks[self.mp_pose.PoseLandmark.RIGHT_SHOULDER].y)

        # è®¡ç®—ä¸­å¿ƒ x åæ ‡ç”¨äºæ’åº
        x_center = (left_shoulder[0] + right_shoulder[0]) / 2

        return Person(
            x_center=x_center,
            left_wrist=left_wrist,
            right_wrist=right_wrist,
            left_shoulder=left_shoulder,
            right_shoulder=right_shoulder,
            person_id=""
        )

    def process_frame(self, frame):
        """å¤„ç†å•å¸§"""
        frame_height, frame_width = frame.shape[:2]
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # æ£€æµ‹å§¿æ€ï¼ˆæ³¨æ„ï¼šMediaPipe Pose ä¸€æ¬¡åªèƒ½æ£€æµ‹ä¸€ä¸ªäººï¼‰
        # ä¸ºäº†ç®€åŒ– demoï¼Œæˆ‘ä»¬å‡è®¾ä½¿ç”¨å¤šæ¬¡æ£€æµ‹æˆ–ä½¿ç”¨å…¶ä»–æ–¹æ³•
        # è¿™é‡Œæä¾›ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦ä½¿ç”¨ BlazePose æˆ–å…¶ä»–å¤šäººæ£€æµ‹æ–¹æ¡ˆ

        results = self.pose.process(rgb_frame)
        person = self.extract_person_data(results, frame_width, frame_height)

        return person

    def run(self, camera_index: int = 0):
        """è¿è¡Œä¸»å¾ªç¯"""
        cap = cv2.VideoCapture(camera_index)

        if not cap.isOpened():
            print("âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
            return

        print("âœ… èº«ä½“å›è·¯ä¹é˜Ÿå·²å¯åŠ¨")
        print("ğŸ“ æç¤ºï¼šéœ€è¦ 3 ä¸ªäººåŒæ—¶ä¸¾æ‰‹å¹¶æ‰‹æ‹‰æ‰‹å½¢æˆå›è·¯")
        print("æŒ‰ 'q' é€€å‡º")

        try:
            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break

                frame = cv2.flip(frame, 1)  # é•œåƒç¿»è½¬
                frame_height, frame_width = frame.shape[:2]

                # TODO: å®é™…åº”ç”¨ä¸­éœ€è¦æ£€æµ‹å¤šä¸ªäºº
                # è¿™é‡Œæ˜¯ç®€åŒ–ç‰ˆæœ¬çš„æ¼”ç¤ºæ¡†æ¶
                person = self.process_frame(frame)

                # æ˜¾ç¤ºæç¤ºä¿¡æ¯
                cv2.putText(frame, "Body Circuit Band - Demo", (10, 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
                cv2.putText(frame, "Note: This demo needs 3 people detection", (10, 70),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)

                cv2.imshow('Body Circuit Band', frame)

                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break

        finally:
            cap.release()
            cv2.destroyAllWindows()
            self.audio_controller.cleanup()
            print("ğŸ‘‹ ç¨‹åºå·²é€€å‡º")


def main():
    # éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆéœ€è¦å‡†å¤‡ä¸‰ä¸ªéŸ³é¢‘æ–‡ä»¶ï¼‰
    drum_path = "audio_samples/drum.wav"
    bass_path = "audio_samples/bass.wav"
    harmony_path = "audio_samples/harmony.wav"

    band = BodyCircuitBand(drum_path, bass_path, harmony_path)
    band.run(camera_index=0)


if __name__ == "__main__":
    main()
